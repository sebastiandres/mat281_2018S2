{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Configuracion para recargar módulos y librerías \n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MAT281\n",
    "\n",
    "## Aplicaciones de la Matemática en la Ingeniería\n",
    "\n",
    "Puedes ejecutar este jupyter notebook de manera interactiva:\n",
    "\n",
    "[![Binder](../shared/images/jupyter_binder.png)](https://mybinder.org/v2/gh/sebastiandres/mat281_m04_data_science/master?filepath=04_clasificacion/04_clasificacion.ipynb)\n",
    "\n",
    "[![Colab](../shared/images/jupyter_colab.png)](https://colab.research.google.com/github/sebastiandres/mat281_m04_data_science/blob/master//04_clasificacion/04_clasificacion.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Qué contenido aprenderemos?\n",
    "* Clasificación\n",
    "* Regresión Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivación\n",
    "* La __clasificación__ es un problema muy común puesto que permite la toma de decisiones. \n",
    "- __Regresión logística__ es un algoritmo que surge naturalmente como extensión de regresión lineal pero en el contexto de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clasficación\n",
    "### Wine Dataset\n",
    "<img src=\"images/wine.jpg\" alt=\"\" width=\"600px\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los datos corresponden a 3 cultivos diferentes de vinos de la misma región de Italia, y que han sido identificados con las etiquetas 1, 2 y 3. Para cada tipo de vino se realizado 13 análisis químicos:\n",
    "\n",
    "1. Alcohol \n",
    "2.  Malic acid \n",
    "3. Ash \n",
    "4. Alcalinity of ash \n",
    "5. Magnesium \n",
    "6. Total phenols \n",
    "7. Flavanoids \n",
    "8. Nonflavanoid phenols \n",
    "9. Proanthocyanins \n",
    "10. Color intensity \n",
    "11. Hue \n",
    "12. OD280/OD315 of diluted wines \n",
    "13. Proline \n",
    "\n",
    "\n",
    "La base de datos contiene 178 muestras distintas en total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Conocemos los valores y las etiquetas, y se desea obtener la etiqueta de una muestra sin etiquetar, por lo que el problema es de __clasificación__:\n",
    "* Tenemos $ X \\in R^{n \\times m} \\textrm{ y } Y \\in N^m $  y buscamos las etiquetas de $ x \\in R^n$.\n",
    "* ¿A qué grupo pertenece este nuevo dato?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regresión Lineal\n",
    "Se buscaba entrenar una función lineal\n",
    "$$h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + ... + \\theta_n x_n$$ de\n",
    "forma que se minimice\n",
    "$$J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regresión Logística\n",
    "Buscaremos entrenar una función\n",
    "logística\n",
    "$$h_{\\theta}(x) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 x_1 + ... + \\theta_n x_n)}}$$\n",
    "\n",
    "de forma que se minimice\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejemplo 2D\n",
    "¿Conocen el accidente del Space Shuttle Challeger?\n",
    "\n",
    "28 Junio 1986. A pesar de existir evidencia de funcionamiento defectuoso, se da luz verde al lanzamiento.\n",
    "\n",
    "<img src=\"images/Challenger1.gif\" alt=\"\" width=\"600px\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A los 73 segundos de vuelo, el transbordador espacial explota, matando a los 7 pasajeros.\n",
    "\n",
    "<img src=\"images/Challenger2.jpg\" alt=\"\" width=\"600px\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejemplo 2D\n",
    "Como parte del debriefing del accidente, se obtuvieron los siguientes datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat data/Challenger.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Grafiquemos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "challenger = pd.DataFrame(np.loadtxt(\"data/Challenger.txt\", skiprows=1).astype(int), columns=[\"temp_f\", \"nm_bad_rings\"])\n",
    "challenger.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(challenger).mark_circle(size=100).encode(\n",
    "    x=alt.X(\"temp_f\", scale=alt.Scale(zero=False), title=\"Temperature [F]\"),\n",
    "    y=alt.Y(\"nm_bad_rings\", title=\"# Bad Rings\")\n",
    ").properties(\n",
    "    title=\"Exito o Falla en lanzamiento de Challenger\",\n",
    "    width=800,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nos gustaría saber en qué condiciones se produce accidente. No nos importa el número de fallas, sólo si existe falla o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger = challenger.assign(temp_c=lambda x: ((x[\"temp_f\"] - 32.) / 1.8).round(2),\n",
    "                               is_failure=lambda x: x[\"nm_bad_rings\"].apply(lambda y: 1 if y==0 else 0),\n",
    "                               ds_failure=lambda x: x[\"is_failure\"].apply(lambda y: \"Falla\" if y==0 else \"Éxito\")\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_chart = alt.Chart(challenger).mark_circle(size=100).encode(\n",
    "    x=alt.X(\"temp_c:Q\", scale=alt.Scale(zero=False), title=\"Temperature [C]\"),\n",
    "    y=alt.Y(\"is_failure:Q\", scale=alt.Scale(padding=0.5), title=\"Success/Failure\"),\n",
    "    color=\"ds_failure:N\"\n",
    ").properties(\n",
    "    title=\"Exito o Falla en lanzamiento de Challenger\",\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "failure_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelo\n",
    "\n",
    "Definimos como\n",
    "antes\n",
    "\n",
    "$$\\begin{aligned}\n",
    "Y &= \\begin{bmatrix}y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)}\\end{bmatrix}\\end{aligned}$$\n",
    "\n",
    "y\n",
    "\n",
    "$$\\begin{aligned}\n",
    "X = \n",
    "\\begin{bmatrix} \n",
    "1 & x^{(1)}_1 & \\dots & x^{(1)}_n \\\\ \n",
    "1 & x^{(2)}_1 & \\dots & x^{(2)}_n \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "1 & x^{(m)}_1 & \\dots & x^{(m)}_n \\\\\n",
    "\\end{bmatrix}\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Luego la\n",
    "evaluación de todos los datos puede escribirse matricialmente como\n",
    "\n",
    "$$\\begin{aligned}\n",
    "X \\theta &= \n",
    "\\begin{bmatrix}\n",
    "1 & x_1^{(1)} & ... & x_n^{(1)} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "1 & x_1^{(m)} & ... & x_n^{(m)} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_n\\end{bmatrix} \\\\\n",
    "& = \n",
    "\\begin{bmatrix}\n",
    "1 \\theta_0 + x^{(1)}_1 \\theta_1 + ... + x^{(1)}_n \\theta_n \\\\\n",
    "\\vdots \\\\\n",
    "1 \\theta_0 + x^{(m)}_1 \\theta_1 + ... + x^{(m)}_n \\theta_n \\\\\n",
    "\\end{bmatrix}\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nuestro problema\n",
    "es encontrar un “buen” conjunto de valores $\\theta$ \n",
    "de modo que\n",
    "\n",
    "$$\\begin{aligned}\n",
    "g(X\\theta)\n",
    "\\approx\n",
    "Y\\end{aligned}$$\n",
    "\n",
    "donde $g(z)$ es la función sigmoide (en. sigmoid function).\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretación gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1. / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-5,5,100)\n",
    "sigmoid_df = pd.DataFrame({\"z\": z,\n",
    "                           \"sigmoid(z)\": sigmoid(z),\n",
    "                           \"sigmoid(z*2)\": sigmoid(z * 2),\n",
    "                           \"sigmoid(z-2)\": sigmoid(z - 2),\n",
    "                          })\n",
    "sigmoid_df = pd.melt(sigmoid_df, id_vars=\"z\", value_vars=[\"sigmoid(z)\", \"sigmoid(z*2)\", \"sigmoid(z-2)\"], var_name=\"sigmoid_function\", value_name=\"value\")\n",
    "sigmoid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(sigmoid_df).mark_line().encode(\n",
    "    x=\"z:Q\",\n",
    "    y=\"value:Q\",\n",
    "    color=\"sigmoid_function:N\"\n",
    ").properties(\n",
    "    title=\"Sigmoid functions\",\n",
    "    width=800,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Modelo\n",
    "## Función Sigmoide\n",
    "\n",
    "La función\n",
    "sigmoide $g(z) = (1+e^{-z})^{-1}$ tiene la siguiente propiedad:\n",
    "$$g'(z) =  g(z)(1-g(z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$g(z) = (1+e^{-z})^{-1}$ y $g'(z) =  g(z)(1-g(z))$.\n",
    "\n",
    "Demostración:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "g'(z) &= \\frac{-1}{(1+e^{-z})^2} (-e^{-z}) \\\\\n",
    "      &= \\frac{e^{-z}}{(1+e^{-z})^2} \\\\\n",
    "      &= \\frac{1}{1+e^{-z}} \\frac{e^{-z}}{1+e^{-z}} \\\\\n",
    "      &= \\frac{1}{1+e^{-z}} \\left(1 - \\frac{1}{1+e^{-z}} \\right) \\\\\n",
    "      &= g(z)(1-g(z))\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretación gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_sigmoid(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "sigmoid_dz_df = pd.DataFrame({\"z\": z,\n",
    "                              \"sigmoid(z)\": sigmoid(z),\n",
    "                              \"d_sigmoid(z)/dz\": d_sigmoid(z)\n",
    "                             }).melt(id_vars=\"z\", value_vars=[\"sigmoid(z)\", \"d_sigmoid(z)/dz\"], var_name=\"function\")\n",
    "sigmoid_dz_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(sigmoid_dz_df).mark_line().encode(\n",
    "    x=\"z:Q\",\n",
    "    y=\"value:Q\",\n",
    "    color=\"function:N\"\n",
    ").properties(\n",
    "    title=\"Sigmoid and her derivate function\",\n",
    "    width=800,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aproximación Ingenieril\n",
    "\n",
    "¿Cómo podemos reutilizar lo que conocemos de regresión lineal?\n",
    "\n",
    "Si buscamos minimizar\n",
    "$$J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$\n",
    "Podemos calcular el gradiente y luego utilizar el método del máximo\n",
    "descenso para obtener $\\theta$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El cálculo del gradiente es directo:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_k}\n",
    "&= \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right) \\frac{\\partial}{\\partial \\theta_k} h_{\\theta}(x^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right) \\frac{\\partial}{\\partial \\theta_k} g(\\theta^T x^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right) h_{\\theta}(x^{(i)}) \\left(1-h_{\\theta}(x^{(i)})\\right) \\frac{\\partial}{\\partial \\theta_k} (\\theta^T x^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right) h_{\\theta}(x^{(i)}) \\left(1-h_{\\theta}(x^{(i)})\\right) x^{(i)}_k\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¿Hay alguna forma de escribir todo esto de manera matricial? Recordemos\n",
    "que si las componentes eran\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right) x^{(i)}_k = \\sum_{i=1}^{m}  x^{(i)}_k \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)\\end{aligned}$$\n",
    "\n",
    "podíamos escribirlo vectorialmente como $$X^T (X\\theta - Y)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Luego, para\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_k}\n",
    "&= \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right) h_{\\theta}(x^{(i)}) \\left(1-h_{\\theta}(x^{(i)})\\right) x^{(i)}_k \\\\\n",
    "&= \\sum_{i=1}^{m}  x^{(i)}_k \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right) h_{\\theta}(x^{(i)}) \\left(1-h_{\\theta}(x^{(i)})\\right)\\end{aligned}$$\n",
    "\n",
    "podemos escribirlo vectorialmente como\n",
    "$$\\nabla_{\\theta} J(\\theta) = X^T  \\Big[ (g(X\\theta) - Y) \\odot g(X\\theta) \\odot (1-g(X\\theta)) \\Big]$$\n",
    "donde $\\odot$ es la multiplicación elemento a elemento (element-wise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Observación crucial:**\n",
    "$$\\nabla_{\\theta} J(\\theta) = X^T  \\Big[ (g(X\\theta) - Y) \\odot g(X\\theta) \\odot (1-g(X\\theta)) \\Big]$$\n",
    "no permite construir un sistema lineal para $\\theta$, por lo cual sólo\n",
    "podemos resolver iterativamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por\n",
    "lo tanto tenemos el algoritmo\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\theta^{(n+1)} & = \\theta^{(n)} - \\alpha \\nabla_{\\theta} J(\\theta^{(n)}) \\\\\n",
    "\\nabla_{\\theta} J(\\theta) &= X^T  \\Big[ (g(X\\theta) - Y) \\odot g(X\\theta) \\odot (1-g(X\\theta)) \\Big]\\end{aligned}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El código sería el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm2_error_logistic_regression(X, Y, theta0, tol=1E-6):\n",
    "    converged = False\n",
    "    alpha = 0.01/len(Y)\n",
    "    theta = theta0\n",
    "    while not converged:\n",
    "        H = sigmoid(np.dot(X, theta))\n",
    "        gradient = np.dot(X.T, (H-Y)*H*(1-H))\n",
    "        new_theta = theta - alpha * gradient\n",
    "        converged = np.linalg.norm(theta-new_theta) < tol * np.linalg.norm(theta) \n",
    "        theta = new_theta\n",
    "    return theta\n",
    "\n",
    "# def sigmoid(z):\n",
    "#     return 1./(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretación Probabilística\n",
    "\n",
    "¿Es la derivación anterior\n",
    "probabilísticamente correcta?\n",
    "\n",
    "Asumamos que la pertenencia a los grupos está dado por\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{P}[y = 1| \\ x ; \\theta ] & = h_\\theta(x) \\\\\n",
    "\\mathbb{P}[y = 0| \\ x ; \\theta ] & = 1 - h_\\theta(x)\\end{aligned}$$\n",
    "\n",
    "Esto es, una distribución de Bernoulli con $p=h_\\theta(x)$.\\\n",
    "Las expresiones anteriores pueden escribirse de manera más compacta como\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{P}[y | \\ x ; \\theta ] & = (h_\\theta(x))^y (1 - h_\\theta(x))^{(1-y)} \\\\\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La función de verosimilitud $L(\\theta)$ nos\n",
    "permite entender que tan probable es encontrar los datos observados,\n",
    "para una elección del parámetro $\\theta$.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "L(\\theta) \n",
    "&= \\prod_{i=1}^{m} \\mathbb{P}[y^{(i)}| x^{(i)}; \\theta ] \\\\\n",
    "&= \\prod_{i=1}^{m} \\Big(h_{\\theta}(x^{(i)})\\Big)^{y^{(i)}} \\Big(1 - h_\\theta(x^{(i)})\\Big)^{(1-y^{(i)})}\\end{aligned}$$\n",
    "\n",
    "Nos gustaría encontrar el parámetro $\\theta$ que más probablemente haya\n",
    "generado los datos observados, es decir, el parámetro $\\theta$ que\n",
    "maximiza la función de verosimilitud.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Calculamos la log-verosimilitud:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "l(\\theta) \n",
    "&= \\log L(\\theta) \\\\\n",
    "&= \\log \\prod_{i=1}^{m} (h_\\theta(x^{(i)}))^{y^{(i)}} (1 - h_\\theta(x^{(i)}))^{(1-y^{(i)})} \\\\\n",
    "&= \\sum_{i=1}^{m} y^{(i)}\\log (h_\\theta(x^{(i)})) + (1-y^{(i)}) \\log (1 - h_\\theta(x^{(i)}))\\end{aligned}$$\n",
    "\n",
    "No existe una fórmula cerrada que nos permita obtener el máximo de la\n",
    "log-verosimitud. Pero podemos utilizar nuevamente el método del\n",
    "gradiente máximo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recordemos que si\n",
    "\n",
    "$$\\begin{aligned}\n",
    "g(z) = \\frac{1}{1+e^{-z}}\\end{aligned}$$\n",
    "\n",
    "Entonces\n",
    "\n",
    "$$\\begin{aligned}\n",
    "g'(z) &= g(z)(1-g(z))\\end{aligned}$$\n",
    "\n",
    "y luego tenemos que\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\theta_k} h_\\theta(x) &= h_\\theta(x) (1-h_\\theta(x)) x_k\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\theta_k} l(\\theta) &=\n",
    "\\frac{\\partial}{\\partial \\theta_k}  \\sum_{i=1}^{m} y^{(i)}\\log (h_\\theta(x^{(i)})) + (1-y^{(i)}) \\log (1 - h_\\theta(x^{(i)})) \\\\\n",
    "&= \\sum_{i=1}^{m} y^{(i)}\\frac{\\partial}{\\partial \\theta_k}   \\log (h_\\theta(x^{(i)})) + (1-y^{(i)}) \\frac{\\partial}{\\partial \\theta_k}  \\log (1 - h_\\theta(x^{(i)})) \\\\\n",
    "&= \\sum_{i=1}^{m} y^{(i)}\\frac{1}{h_\\theta(x^{(i)})}\\frac{\\partial h_\\theta(x^{(i)})}{\\partial \\theta_k} \n",
    "+ (1-y^{(i)}) \\frac{1}{1 - h_\\theta(x^{(i)})} \\frac{\\partial (1-h_\\theta(x^{(i)}))}{\\partial \\theta_k} \\\\\n",
    "&= \\sum_{i=1}^{m} y^{(i)}(1-h_\\theta(x^{(i)})) x^{(i)}- (1-y^{(i)}) h_\\theta(x^{(i)}) x^{(i)}\\\\\n",
    "&= \\sum_{i=1}^{m} y^{(i)}x^{(i)}- y^{(i)}h_\\theta(x^{(i)}) x^{(i)}- h_\\theta(x^{(i)}) x^{(i)}+ y^{(i)}h_\\theta(x^{(i)}) x^{(i)}\\\\\n",
    "&= \\sum_{i=1}^{m} (y^{(i)}-h_\\theta(x^{(i)})) x^{(i)}\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es decir, para maximizar la log-verosimilitud\n",
    "obtenemos igual que para la regresión lineal:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\theta^{(n+1)} & = \\theta^{(n)} - \\alpha \\nabla_{\\theta} l(\\theta^{(n)}) \\\\\n",
    "\\frac{\\partial l(\\theta)}{\\partial \\theta_k}\n",
    "&= \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right) x^{(i)}_k\\end{aligned}$$\n",
    "\n",
    "Aunque, en el caso de regresión logística, se tiene\n",
    "$h_\\theta(x)=1/(1+e^{-x^T\\theta})$\n",
    "\n",
    "OBS: La elección de $\\alpha$ es crucial para la convergencia. En\n",
    "particular, $0.01/m$ funciona bien.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recuerdo de la aproximación Ingenieril\n",
    "\n",
    "Por\n",
    "lo tanto tenemos el algoritmo\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\theta^{(n+1)} & = \\theta^{(n)} - \\alpha \\nabla_{\\theta} J(\\theta^{(n)}) \\\\\n",
    "\\nabla_{\\theta} J(\\theta) &= X^T  \\Big[ (g(X\\theta) - Y) \\odot g(X\\theta) \\odot (1-g(X\\theta)) \\Big]\\end{aligned}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretación Probabilística\n",
    "\n",
    "Es decir, para maximizar la log-verosimilitud\n",
    "obtenemos igual que para la regresión lineal:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\theta^{(n+1)} & = \\theta^{(n)} - \\alpha \\nabla_{\\theta} l(\\theta^{(n)}) \\\\\n",
    "\\frac{\\partial l(\\theta)}{\\partial \\theta_k}\n",
    "&= \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right) x^{(i)}_k\\end{aligned}$$\n",
    "\n",
    "Aunque, en el caso de regresión logística, se tiene\n",
    "$h_\\theta(x)=1/(1+e^{-x^T\\theta})$\n",
    "\n",
    "OBS: La elección de $\\alpha$ es crucial para la convergencia. En\n",
    "particular, $0.01/m$ funciona bien.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def likelihood_logistic_regression(X, Y, theta0, tol=1E-6):\n",
    "    converged = False\n",
    "    alpha = 0.01/len(Y)\n",
    "    theta = theta0\n",
    "    while not converged:\n",
    "        H = sigmoid(np.dot(X, theta))\n",
    "        gradient = np.dot(X.T, H-Y)\n",
    "        new_theta = theta - alpha * gradient\n",
    "        converged = np.linalg.norm(theta-new_theta) < tol * np.linalg.norm(theta) \n",
    "        theta = new_theta\n",
    "    return theta\n",
    "\n",
    "# def sigmoid(z):\n",
    "#     return 1. / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretación del resultado\n",
    "\n",
    "* ¿Qué significa el parámetro obtenido $\\theta$?\n",
    "* ¿Cómo relacionamos la pertenencia a una clase (discreto) con la hipótesis $h_{\\theta}(x)$ (continuo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Aplicación a Datos del Challenger\n",
    "\n",
    "Apliquemos lo anterior a los datos que tenemos del Challenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = challenger.loc[:, [\"temp_c\"]].assign(intercept=1)[[\"intercept\", \"temp_c\"]].values\n",
    "y = challenger[\"is_failure\"].values\n",
    "theta_0 = y.mean() / X.mean(axis=0)\n",
    "print(f\"theta_0 = {theta_0}\")\n",
    "theta_J = norm2_error_logistic_regression(X, y, theta_0)\n",
    "print(f\"theta_J = {theta_J}\")\n",
    "theta_l = likelihood_logistic_regression(X, y, theta_0)\n",
    "print(f\"theta_l = {theta_l}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Visualización de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = pd.DataFrame({\"temp_c\": X[:, 1],\n",
    "                           \"norm_2_error_prediction\": sigmoid(np.dot(X, theta_J)),\n",
    "                           \"likelihood_prediction\": sigmoid(np.dot(X, theta_l))\n",
    "                          }).melt(id_vars=\"temp_c\", value_vars=[\"norm_2_error_prediction\", \"likelihood_prediction\"], var_name=\"prediction\")\n",
    "predict_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_chart = alt.Chart(predict_df).mark_line().encode(\n",
    "    x=alt.X(\"temp_c:Q\", scale=alt.Scale(zero=False), title=\"Temperature [C]\"),\n",
    "    y=alt.Y(\"value:Q\", scale=alt.Scale(padding=0.5)),\n",
    "    color=\"prediction:N\"\n",
    ")\n",
    "(prediction_chart + failure_chart).properties(title=\"Data and Prediction Failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Aplicación al Iris Dataset\n",
    "\n",
    "Hay\n",
    "definidas $3$ clases, pero nosotros sólo podemos clasificar en $2$ clases. ¿Qué hacer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "# Loading the data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "print(iris.target_names)\n",
    "\n",
    "# Print data and labels\n",
    "for x, y in zip(X,Y):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Podemos ***definir*** 2 clases: Iris Setosa y no Iris Setosa.\n",
    "\n",
    "¿Que label le pondremos a cada clase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(iris.target==0, int)\n",
    "# Print data and labels\n",
    "for x, y in zip(X,Y):\n",
    "    print (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para aplicar el algoritmo, utilizando el algoritmo [Logistic Regression](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) de la librería sklearn, requerimos un código como el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Fitting the model\n",
    "Logit = LogisticRegression()\n",
    "Logit.fit(X,Y)\n",
    "\n",
    "# Obtain the coefficients\n",
    "print(Logit.intercept_, Logit.coef_ )\n",
    "\n",
    "# Predicting values\n",
    "Y_pred = Logit.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Podemos visualizar el resultado con una matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(Y, Y_pred)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**¡Nuestra clasificación es perfecta!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Referencias\n",
    "\n",
    "* Jake VanderPlas, ESAC Data Analysis and Statistics Workshop 2014, https://github.com/jakevdp/ESAC-stats-2014\n",
    "* Andrew Ng, Machine Learning CS144, Stanford University."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
