{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAT281\n",
    "## Aplicaciones de la Matemática en la Ingeniería"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración para plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Intrucciones__\n",
    "\n",
    "* Completa tus datos personales (nombre y rol USM).\n",
    "* Debes enviar este .ipynb con el siguiente formato de nombre: 08_lab_clasificacion_NOMBRE_APELLIDO.ipynb con tus respuestas a alonso.ogueda@gmail.com y sebastian.flores@usm.cl .\n",
    "* Se evaluará:\n",
    "    - Soluciones\n",
    "    - Código\n",
    "    - Al presionar  `Kernel -> Restart Kernel and Run All Cells` deben ejecutarse todas las celdas sin error.\n",
    "    - La escala es de 0 a 4 considerando solo valores enteros.\n",
    "* __La entrega es al final de esta clase.__\n",
    "\n",
    "__Nombre__:\n",
    "\n",
    "__Rol__:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observación\n",
    "\n",
    "Este laboratorio utiliza la librería sklearn (oficialmente llamada [scikit learn](http://scikit-learn.org/stable/)), de la cual utilizaremos el método de clasificación **k Nearest Neighbors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema: clasificación de dígitos\n",
    "En este laboratorio realizaremos el trabajo de reconocer un dígito a partir de una imagen.\n",
    "\n",
    "El repositorio con los datos se encuentra en el siguiente [link](https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits), pero los datos ya han sido incluídos en el directorio `data/`. \n",
    "\n",
    "## Contenido\n",
    "\n",
    "El laboratorio consiste de 4 secciones:\n",
    "0. Explicación de k Neirest Neighbours\n",
    "1. Exploración de los datos.\n",
    "2. Entrenando el modelo kNN.\n",
    "3. Estimación del error de predicción de dígitos utilizando kNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Qué es k Neirest Neighbours?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo **k Nearest Neighbors** es un método no paramétrico: una vez que el parámetro $k$ se ha fijado, no se busca obtener ningún parámetro adicional.\n",
    "\n",
    "Sean los puntos $x^{(i)} = (x^{(i)}_1, ..., x^{(i)}_n)$  de etiqueta $y^{(i)}$ conocida, para $i=1, ..., m$.\n",
    "\n",
    "El problema de clasificación consiste en encontrar la etiqueta de un nuevo punto $x=(x_1, ..., x_m)$ para el cual no conocemos la etiqueta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La etiqueta de un punto se obtiene de la siguiente forma:\n",
    "* Para $k=1$, **1NN** asigna a $x$ la etiqueta de su vecino más cercano. \n",
    "* Para $k$ genérico, **kNN** asigna a $x$ la etiqueta más popular de los k vecinos más cercanos. \n",
    "\n",
    "El modelo subyacente a kNN es el conjunto de entrenamiento completo. A diferencia de otros métodos que efectivamente generalizan y resumen la información (como regresión logística, por ejemplo), cuando se necesita realizar una predicción, el algoritmo kNN mira **todos** los datos y selecciona los k datos más cercanos, para regresar la etiqueta más popular/más común. Los datos no se resumen en parámetros, sino que siempre deben mantenerse en memoria. Es un método por tanto que no escala bien con un gran número de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de empate, existen diversas maneras de desempatar:\n",
    "* Elegir la etiqueta del vecino más cercano (problema: no garantiza solución).\n",
    "* Elegir la etiqueta de menor valor (problema: arbitrario).\n",
    "* Elegir la etiqueta que se obtendría con $k+1$ o $k-1$ (problema: no garantiza solución, aumenta tiempo de cálculo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cercanía o similaridad entre los datos se mide de diversas maneras, pero en general depende del tipo de datos y del contexto.\n",
    "\n",
    "* Para datos reales, puede utilizarse cualquier distancia, siendo la **distancia euclidiana** la más utilizada. También es posible ponderar unas componentes más que otras. Resulta conveniente normalizar para poder utilizar la noción de distancia más naturalmente.\n",
    "\n",
    "* Para **datos categóricos o binarios**, suele utilizarse la distancia de Hamming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, una implementación de \"bare bones\" en numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediccion realizada para etiqueta del punto = 0 (circulo azul)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Parámetros\n",
    "k = 3\n",
    "\n",
    "def knn_search(X, k, x):\n",
    "    \"\"\" find K nearest neighbours of data among D \"\"\"\n",
    "    # Distancia euclidiana\n",
    "    d = np.sqrt(((X - x[:,:k])**2).sum(axis=0))\n",
    "    # Ordenar por cercania\n",
    "    idx = np.argsort(d)\n",
    "    # Regresar los k mas cercanos\n",
    "    id_closest = idx[:k] \n",
    "    return id_closest, d[id_closest].max()\n",
    "\n",
    "def knn(X,Y,k,x):\n",
    "    # Obtener los k mas cercanos\n",
    "    k_closest, dmax = knn_search(X, k, x)\n",
    "    # Obtener las etiquetas\n",
    "    Y_closest = Y[k_closest]\n",
    "    # Obtener la mas popular\n",
    "    counts = np.bincount(Y_closest.flatten())\n",
    "    #print(counts)\n",
    "    # Regresar la mas popular (cualquiera, si hay empate)\n",
    "    return np.argmax(counts)\n",
    "\n",
    "N = 100\n",
    "X = np.random.rand(2,N) # random dataset\n",
    "Y = np.array(np.random.rand(N)<0.4, dtype=int).reshape((N,1)) # random dataset\n",
    "x = np.random.rand(2,1) # query point\n",
    "\n",
    "# performing the search\n",
    "neig_idx, dmax = knn_search(X, k, x)\n",
    "y = knn(X, Y, k, x)    \n",
    "    \n",
    "# plotting the data and the input point\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.plot(x[0,0],x[1,0],'ok', ms=16)\n",
    "m_ob = Y[:,0]==0\n",
    "plt.plot(X[0,m_ob], X[1,m_ob], 'ob', ms=8)\n",
    "m_sr = Y[:,0]==1\n",
    "plt.plot(X[0,m_sr],X[1,m_sr],'sr', ms=8)\n",
    "\n",
    "# highlighting the neighbours\n",
    "plt.plot(X[0,neig_idx], X[1,neig_idx], 'o', markerfacecolor='None', markersize=24, markeredgewidth=1)\n",
    "\n",
    "# Plot a circle\n",
    "x_circle = dmax*np.cos(np.linspace(0,2*np.pi,360)) +  x[0,0]\n",
    "y_circle = dmax*np.sin(np.linspace(0,2*np.pi,360)) +  x[1,0]\n",
    "plt.plot(x_circle, y_circle, 'k', alpha=0.25)\n",
    "# Show all\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.show()\n",
    "\n",
    "# Print result\n",
    "if y==0:\n",
    "  print(\"Prediccion realizada para etiqueta del punto = {} (circulo azul)\".format(y))\n",
    "else:\n",
    "  print(\"Prediccion realizada para etiqueta del punto = {} (cuadrado rojo)\".format(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes ejecutar varias veces el código anterior, variando el número de vecinos `k` para ver cómo afecta el algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploración de los datos\n",
    "\n",
    "Los datos se encuentran en 2 archivos, `data/optdigits.train` y `data/optdigits.test`. Como su nombre lo indica, el set `data/optdigits.train` contiene los ejemplos que deben ser usados para entrenar el modelo, mientras que el set `data/optdigits.test` se utilizará para obtener una estimación del error de predicción.\n",
    "\n",
    "Ambos archivos comparten el mismo formato: cada línea contiene 65 valores. Los 64 primeros corresponden a la representación de la imagen en escala de grises (0-blanco, 255-negro), y el valor 65 corresponde al dígito de la imagen (0-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código para mostrar los archivos en un directorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optdigits.names.txt\n",
      "optdigits.test\n",
      "optdigits.train\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código para revisar un archivo (reemplazar por cualquiera de los archivos de interés)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Title of Database: Optical Recognition of Handwritten Digits\n",
      "\n",
      "2. Source:\n",
      "\tE. Alpaydin, C. Kaynak\n",
      "\tDepartment of Computer Engineering\n",
      "\tBogazici University, 80815 Istanbul Turkey\n",
      "\talpaydin@boun.edu.tr\n",
      "\tJuly 1998\n",
      "\n",
      "3. Past Usage:\n",
      "\tC. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "\tApplications to Handwritten Digit Recognition, \n",
      "\tMSc Thesis, Institute of Graduate Studies in Science and \n",
      "\tEngineering, Bogazici University.\n",
      "\n",
      "\tE. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika,\n",
      "\tto appear. ftp://ftp.icsi.berkeley.edu/pub/ai/ethem/kyb.ps.Z\n",
      "\n",
      "4. Relevant Information:\n",
      "\tWe used preprocessing programs made available by NIST to extract\n",
      "\tnormalized bitmaps of handwritten digits from a preprinted form. From\n",
      "\ta total of 43 people, 30 contributed to the training set and different\n",
      "\t13 to the test set. 32x32 bitmaps are divided into nonoverlapping \n",
      "\tblocks of 4x4 and the number of on pixels are counted in each block.\n",
      "\tThis generates an input matrix of 8x8 where each element is an \n",
      "\tinteger in the range 0..16. This reduces dimensionality and gives \n",
      "\tinvariance to small distortions.\n",
      "\n",
      "\tFor info on NIST preprocessing routines, see \n",
      "\tM. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, \n",
      "\tP. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based \n",
      "\tHandprint Recognition System, NISTIR 5469, 1994.\n",
      "\n",
      "5. Number of Instances\n",
      "\toptdigits.tra\tTraining\t3823\n",
      "\toptdigits.tes\tTesting\t\t1797\n",
      "\t\n",
      "\tThe way we used the dataset was to use half of training for \n",
      "\tactual training, one-fourth for validation and one-fourth\n",
      "\tfor writer-dependent testing. The test set was used for \n",
      "\twriter-independent testing and is the actual quality measure.\n",
      "\n",
      "6. Number of Attributes\n",
      "\t64 input+1 class attribute\n",
      "\n",
      "7. For Each Attribute:\n",
      "\tAll input attributes are integers in the range 0..16.\n",
      "\tThe last attribute is the class code 0..9\n",
      "\n",
      "8. Missing Attribute Values\n",
      "\tNone\n",
      "\n",
      "9. Class Distribution\n",
      "\tClass:\tNo of examples in training set\n",
      "\t0:  376\n",
      "\t1:  389\n",
      "\t2:  380\n",
      "\t3:  389\n",
      "\t4:  387\n",
      "\t5:  376\n",
      "\t6:  377\n",
      "\t7:  387\n",
      "\t8:  380\n",
      "\t9:  382\n",
      "\n",
      "\tClass: No of examples in testing set\n",
      "\t0:  178\n",
      "\t1:  182\n",
      "\t2:  177\n",
      "\t3:  183\n",
      "\t4:  181\n",
      "\t5:  182\n",
      "\t6:  181\n",
      "\t7:  179\n",
      "\t8:  174\n",
      "\t9:  180\n",
      "\n",
      "Accuracy on the testing set with k-nn \n",
      "using Euclidean distance as the metric\n",
      "\n",
      " k =  1   : 98.00\n",
      " k =  2   : 97.38\n",
      " k =  3   : 97.83\n",
      " k =  4   : 97.61\n",
      " k =  5   : 97.89\n",
      " k =  6   : 97.77\n",
      " k =  7   : 97.66\n",
      " k =  8   : 97.66\n",
      " k =  9   : 97.72\n",
      " k = 10   : 97.55\n",
      " k = 11   : 97.89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat data/optdigits.names.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código para mostrar las primeras líneas del archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,6,15,12,1,0,0,0,7,16,6,6,10,0,0,0,8,16,2,0,11,2,0,0,5,16,3,0,5,7,0,0,7,13,3,0,8,7,0,0,4,12,0,1,13,5,0,0,0,14,9,15,9,0,0,0,0,6,14,7,1,0,0,0\n",
      "0,0,10,16,6,0,0,0,0,7,16,8,16,5,0,0,0,11,16,0,6,14,3,0,0,12,12,0,0,11,11,0,0,12,12,0,0,8,12,0,0,7,15,1,0,13,11,0,0,0,16,8,10,15,3,0,0,0,10,16,15,3,0,0,0\n",
      "0,0,8,15,16,13,0,0,0,1,11,9,11,16,1,0,0,0,0,0,7,14,0,0,0,0,3,4,14,12,2,0,0,1,16,16,16,16,10,0,0,2,12,16,10,0,0,0,0,0,2,16,4,0,0,0,0,0,9,14,0,0,0,0,7\n",
      "0,0,0,3,11,16,0,0,0,0,5,16,11,13,7,0,0,3,15,8,1,15,6,0,0,11,16,16,16,16,10,0,0,1,4,4,13,10,2,0,0,0,0,0,15,4,0,0,0,0,0,3,16,0,0,0,0,0,0,1,15,2,0,0,4\n",
      "0,0,5,14,4,0,0,0,0,0,13,8,0,0,0,0,0,3,14,4,0,0,0,0,0,6,16,14,9,2,0,0,0,4,16,3,4,11,2,0,0,0,14,3,0,4,11,0,0,0,10,8,4,11,12,0,0,0,4,12,14,7,0,0,6\n",
      "0,0,11,16,10,1,0,0,0,4,16,10,15,8,0,0,0,4,16,3,11,13,0,0,0,1,14,6,9,14,0,0,0,0,0,0,12,10,0,0,0,0,0,6,16,6,0,0,0,0,5,15,15,8,8,3,0,0,10,16,16,16,16,6,2\n",
      "0,0,1,11,13,11,7,0,0,0,9,14,6,4,3,0,0,0,16,12,16,15,2,0,0,5,16,10,4,12,6,0,0,1,1,0,0,10,4,0,0,0,0,0,5,10,0,0,0,0,0,8,15,3,0,0,0,0,1,13,5,0,0,0,5\n",
      "0,0,8,10,8,7,2,0,0,1,15,14,12,12,4,0,0,7,15,12,5,0,0,0,0,5,14,12,15,7,0,0,0,0,0,0,2,13,0,0,0,0,0,0,4,12,0,0,0,0,6,7,14,5,0,0,0,0,4,13,8,0,0,0,5\n",
      "0,0,15,2,14,13,2,0,0,0,16,15,12,13,8,0,0,2,16,12,1,6,10,0,0,7,15,3,0,5,8,0,0,5,12,0,0,8,8,0,0,5,12,0,7,15,5,0,0,5,16,13,16,6,0,0,0,0,10,12,5,0,0,0,0\n",
      "0,0,3,13,13,2,0,0,0,6,16,12,10,8,0,0,0,9,15,12,16,6,0,0,0,10,16,16,13,0,0,0,0,1,12,16,12,14,4,0,0,0,11,8,0,3,12,0,0,0,13,11,8,13,12,0,0,0,3,15,11,6,0,0,8\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head data/optdigits.train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Cargando los datos en numpy\n",
    "\n",
    "Para cargar los datos, utilizamos np.loadtxt con los parámetros extra delimiter (para indicar que el separador será en esta ocasión una coma) y con el dype np.int8 (para que su representación en memoria sea la mínima posible, 8 bits en vez de 32/64 bits para un float)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  6 ...  0  0  0]\n",
      " [ 0  0 10 ...  0  0  0]\n",
      " [ 0  0  8 ...  0  0  7]\n",
      " ...\n",
      " [ 0  0  3 ...  0  0  6]\n",
      " [ 0  0  6 ...  5  0  6]\n",
      " [ 0  0  2 ...  0  0  7]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "filepath = os.path.join(\"data\", \"optdigits.train\")\n",
    "XY_tv = np.loadtxt(filepath, delimiter=\",\", dtype=np.int8)\n",
    "print(XY_tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3823, 64)\n",
      "(3823,)\n",
      "[ 0  1  6 15 12  1  0  0  0  7 16  6  6 10  0  0  0  8 16  2  0 11  2  0\n",
      "  0  5 16  3  0  5  7  0  0  7 13  3  0  8  7  0  0  4 12  0  1 13  5  0\n",
      "  0  0 14  9 15  9  0  0  0  0  6 14  7  1  0  0]\n",
      "[[ 0  1  6 15 12  1  0  0]\n",
      " [ 0  7 16  6  6 10  0  0]\n",
      " [ 0  8 16  2  0 11  2  0]\n",
      " [ 0  5 16  3  0  5  7  0]\n",
      " [ 0  7 13  3  0  8  7  0]\n",
      " [ 0  4 12  0  1 13  5  0]\n",
      " [ 0  0 14  9 15  9  0  0]\n",
      " [ 0  0  6 14  7  1  0  0]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Split into X (values) and Y (labels)\n",
    "X_tv = XY_tv[:,:64]\n",
    "Y_tv = XY_tv[:, 64]\n",
    "# Some printings\n",
    "print(X_tv.shape)\n",
    "print(Y_tv.shape)\n",
    "print(X_tv[0,:])\n",
    "print(X_tv[0,:].reshape(8,8))\n",
    "print(Y_tv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 (bis) Cargando los datos en pandas\n",
    "\n",
    "Para cargar los datos, también sería posible utilizar pandas. Sin embargo, como el archivo no tiene encabezado, necesitaremos crear nombres para las columnas, además de indicar el separador y tipo de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      c00  c01  c02  c03  c04  c05  c06  c07  c08  c09  ...  c55  c56  c57  \\\n",
      "0       0    1    6   15   12    1    0    0    0    7  ...    0    0    0   \n",
      "1       0    0   10   16    6    0    0    0    0    7  ...    0    0    0   \n",
      "2       0    0    8   15   16   13    0    0    0    1  ...    0    0    0   \n",
      "3       0    0    0    3   11   16    0    0    0    0  ...    0    0    0   \n",
      "4       0    0    5   14    4    0    0    0    0    0  ...    0    0    0   \n",
      "5       0    0   11   16   10    1    0    0    0    4  ...    3    0    0   \n",
      "6       0    0    1   11   13   11    7    0    0    0  ...    0    0    0   \n",
      "7       0    0    8   10    8    7    2    0    0    1  ...    0    0    0   \n",
      "8       0    0   15    2   14   13    2    0    0    0  ...    0    0    0   \n",
      "9       0    0    3   13   13    2    0    0    0    6  ...    0    0    0   \n",
      "10      0    0    6   14   14   16   16    8    0    0  ...    0    0    0   \n",
      "11      0    0    0    3   16   11    1    0    0    0  ...    0    0    0   \n",
      "12      0    0    0    4   13   16   16    3    0    0  ...    0    0    0   \n",
      "13      0    0    7   12    6    2    0    0    0    0  ...    0    0    0   \n",
      "14      0    0    7   11   11    6    0    0    0    9  ...    0    0    0   \n",
      "15      0    1   10   15    8    0    0    0    0    6  ...    0    0    0   \n",
      "16      0    0    0    1   11    7    0    0    0    0  ...    0    0    0   \n",
      "17      0    0    5   12   16   16    3    0    0    0  ...    0    0    0   \n",
      "18      0    0    1    8   13   13    2    0    0    4  ...    0    0    0   \n",
      "19      0    0    0    2   13   12    4    0    0    0  ...    0    0    0   \n",
      "20      0    0    4   11   15   16   15    0    0    0  ...    0    0    0   \n",
      "21      0    0    4   10   13   11    1    0    0    2  ...    0    0    0   \n",
      "22      0    0    3   11   13   14    6    0    0    0  ...    0    0    0   \n",
      "23      0    0    1    4   11   13    7    0    0    2  ...    0    0    0   \n",
      "24      0    0    9   13    1    0    0    0    0    0  ...    5    0    0   \n",
      "25      0    0    9   16   11    0    0    0    0    4  ...    1    0    0   \n",
      "26      0    0    2   13    9    0    0    0    0    0  ...    0    0    0   \n",
      "27      0    0    0   10   12    0    0    0    0    0  ...    0    0    0   \n",
      "28      0    0    0    0   10   13    0    0    0    0  ...    0    0    0   \n",
      "29      0    0    7    9   13   11    2    0    0    6  ...    0    0    0   \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "3793    0    0   15   15   16   14    1    0    0    3  ...    0    0    0   \n",
      "3794    0    0    2   13   16   15    4    0    0    0  ...    0    0    0   \n",
      "3795    0    0   12   16    7    0    0    0    0    2  ...    0    0    0   \n",
      "3796    0    0    0    3   13    0    0    0    0    0  ...    0    0    0   \n",
      "3797    0    0    0   12    8    0    0    0    0    0  ...    0    0    0   \n",
      "3798    0    0    0    9   12    0    0    0    0    0  ...    0    0    0   \n",
      "3799    0    0    5   16   13    1    0    0    0    0  ...    0    0    1   \n",
      "3800    0    0    6   16    8    0    0    0    0    2  ...    0    0    0   \n",
      "3801    0    0    5   16   11    0    0    0    0    1  ...    0    0    0   \n",
      "3802    0    1   12   16   14    4    0    0    0    8  ...    0    0    0   \n",
      "3803    0    0    3   14   13    3    0    0    0    0  ...    0    0    0   \n",
      "3804    0    0   12   16   16   10    1    0    0    0  ...    0    0    1   \n",
      "3805    0    0    0    8   11    0    0    0    0    0  ...    0    0    0   \n",
      "3806    0    1    9   16   14    6    0    0    0    5  ...    0    0    0   \n",
      "3807    0    0    4   14   15    3    0    0    0    0  ...    0    0    0   \n",
      "3808    0    2   15   16   15    1    0    0    0    3  ...    0    0    0   \n",
      "3809    0    0    5   15   16    6    0    0    0    0  ...    0    0    0   \n",
      "3810    0    0    4   16   11    1    0    0    0    0  ...    0    0    0   \n",
      "3811    0    0    0    0    7   11    1    0    0    0  ...    0    0    0   \n",
      "3812    0    0    0    6   13    0    0    0    0    0  ...    0    0    0   \n",
      "3813    0    0    0    6    8    0    0    0    0    0  ...    0    0    1   \n",
      "3814    0    0    9   16    6    0    0    0    0    2  ...    0    0    0   \n",
      "3815    0    0    9   16   12    1    0    0    0    3  ...    0    0    0   \n",
      "3816    0    1   10   16   16    4    0    0    0    8  ...    0    0    2   \n",
      "3817    0    0    6   16   11    0    0    0    0    1  ...    0    0    1   \n",
      "3818    0    0    5   13   11    2    0    0    0    2  ...    0    0    0   \n",
      "3819    0    0    0    1   12    1    0    0    0    0  ...    0    0    0   \n",
      "3820    0    0    3   15    0    0    0    0    0    0  ...    0    0    0   \n",
      "3821    0    0    6   16    2    0    0    0    0    0  ...    0    0    0   \n",
      "3822    0    0    2   15   16   13    1    0    0    0  ...    0    0    0   \n",
      "\n",
      "      c58  c59  c60  c61  c62  c63  c64  \n",
      "0       6   14    7    1    0    0    0  \n",
      "1      10   16   15    3    0    0    0  \n",
      "2       9   14    0    0    0    0    7  \n",
      "3       0    1   15    2    0    0    4  \n",
      "4       4   12   14    7    0    0    6  \n",
      "5      10   16   16   16   16    6    2  \n",
      "6       1   13    5    0    0    0    5  \n",
      "7       4   13    8    0    0    0    5  \n",
      "8      10   12    5    0    0    0    0  \n",
      "9       3   15   11    6    0    0    8  \n",
      "10     10   12    0    0    0    0    7  \n",
      "11      0    2   14   14    1    0    1  \n",
      "12      0    5   15    4    0    0    9  \n",
      "13      5   16    9    0    0    0    5  \n",
      "14     14   16   12   10    1    0    3  \n",
      "15      9   15    8    0    0    0    0  \n",
      "16      0    3   15    0    0    0    4  \n",
      "17      8   12    0    0    0    0    7  \n",
      "18      1   13   12    4    0    0    8  \n",
      "19      0    0   15    3    0    0    4  \n",
      "20      6   14    2    0    0    0    7  \n",
      "21      6   13   11    1    0    0    8  \n",
      "22      3   13   10    0    0    0    5  \n",
      "23      0    1   14    3    0    0    9  \n",
      "24      4   15   16   16   16   16    1  \n",
      "25     10   16    9    9   13    6    2  \n",
      "26      3   16   14    4    0    0    0  \n",
      "27      1   11   14   12    1    0    6  \n",
      "28      0    0    8   15    2    0    1  \n",
      "29     13   12    8    1    0    0    8  \n",
      "...   ...  ...  ...  ...  ...  ...  ...  \n",
      "3793   15   16   12    0    0    0    5  \n",
      "3794    3   14    1    0    0    0    7  \n",
      "3795   11   16   16   16   16    8    2  \n",
      "3796    0    6   12    0    0    0    4  \n",
      "3797    0   14    6    0    0    0    4  \n",
      "3798    0   10   11    0    0    0    4  \n",
      "3799    6   10   12   12    2    0    9  \n",
      "3800    7   13   16   13    9    0    9  \n",
      "3801    4   14   12    2    0    0    0  \n",
      "3802   13   15   15   10    2    0    3  \n",
      "3803    3   13   13    6    0    0    0  \n",
      "3804   10   16   16   16    7    0    3  \n",
      "3805    0    7   12    0    0    0    4  \n",
      "3806   11   16    7    0    0    0    8  \n",
      "3807    3   14   14    6    0    0    0  \n",
      "3808   14   13   16   11    1    0    3  \n",
      "3809    5   15   16   15    1    0    0  \n",
      "3810    2   12   16   11    1    0    1  \n",
      "3811    2    0    7   11    0    0    1  \n",
      "3812    0    6    9    0    0    0    4  \n",
      "3813    4    9    8    0    0    0    4  \n",
      "3814   10   13    1    0    0    0    8  \n",
      "3815    8   16   16   16    8    0    9  \n",
      "3816   13   16   12    5    0    0    3  \n",
      "3817    7   14   16   12    1    0    9  \n",
      "3818    8   13   15   10    1    0    9  \n",
      "3819    0    4    9    0    0    0    4  \n",
      "3820    4   14   16    9    0    0    6  \n",
      "3821    5   16   16   16    5    0    6  \n",
      "3822    4   14    1    0    0    0    7  \n",
      "\n",
      "[3823 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "filepath = os.path.join(\"data\", \"optdigits.train\")\n",
    "col_names = [\"c{:02d}\".format(i) for i in range(65)]\n",
    "df_XY_tv = pd.read_csv(filepath, names=col_names, sep=\",\", dtype=np.int8)\n",
    "print(df_XY_tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3823, 64)\n",
      "(3823,)\n",
      "[ 0  1  6 15 12  1  0  0  0  7 16  6  6 10  0  0  0  8 16  2  0 11  2  0\n",
      "  0  5 16  3  0  5  7  0  0  7 13  3  0  8  7  0  0  4 12  0  1 13  5  0\n",
      "  0  0 14  9 15  9  0  0  0  0  6 14  7  1  0  0]\n",
      "[[ 0  1  6 15 12  1  0  0]\n",
      " [ 0  7 16  6  6 10  0  0]\n",
      " [ 0  8 16  2  0 11  2  0]\n",
      " [ 0  5 16  3  0  5  7  0]\n",
      " [ 0  7 13  3  0  8  7  0]\n",
      " [ 0  4 12  0  1 13  5  0]\n",
      " [ 0  0 14  9 15  9  0  0]\n",
      " [ 0  0  6 14  7  1  0  0]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Split into X (values) and Y (labels)\n",
    "XY_tv = df_XY_tv.values\n",
    "X_tv = XY_tv[:,:64]\n",
    "Y_tv = XY_tv[:, 64]\n",
    "# Some printings\n",
    "print(X_tv.shape)\n",
    "print(Y_tv.shape)\n",
    "print(X_tv[0,:])\n",
    "print(X_tv[0,:].reshape(8,8))\n",
    "print(Y_tv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya hemos mencionado anteriormente, pandas tiene la ventaja de poseer el método `describe` para obtener un rápido resumen de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c00</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c01</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>0.301334</td>\n",
       "      <td>0.866986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c02</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>5.481821</td>\n",
       "      <td>4.631601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c03</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>11.805912</td>\n",
       "      <td>4.259811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c04</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>11.451478</td>\n",
       "      <td>4.537556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c05</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>5.505362</td>\n",
       "      <td>5.613060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c06</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>1.387392</td>\n",
       "      <td>3.371444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c07</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>0.142297</td>\n",
       "      <td>1.051598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c08</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.088572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c09</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>1.960502</td>\n",
       "      <td>3.052353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c10</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>10.577295</td>\n",
       "      <td>5.435481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c11</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>11.715407</td>\n",
       "      <td>4.012160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c12</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>10.624902</td>\n",
       "      <td>4.788136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c13</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>8.295579</td>\n",
       "      <td>5.935551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c14</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>2.200105</td>\n",
       "      <td>4.062178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c15</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>0.151975</td>\n",
       "      <td>0.988778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c16</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>0.004970</td>\n",
       "      <td>0.119857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c17</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>2.595867</td>\n",
       "      <td>3.454065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c18</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>9.580696</td>\n",
       "      <td>5.886126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c19</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>6.735025</td>\n",
       "      <td>5.918303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c20</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>7.186503</td>\n",
       "      <td>6.142687</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c21</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>8.048391</td>\n",
       "      <td>6.291498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c22</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>2.046037</td>\n",
       "      <td>3.581740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c23</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>0.049176</td>\n",
       "      <td>0.435462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c24</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.032334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c25</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>2.335600</td>\n",
       "      <td>3.085915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c26</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>9.239079</td>\n",
       "      <td>6.128091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c27</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>9.133665</td>\n",
       "      <td>5.902591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c28</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>9.673293</td>\n",
       "      <td>6.282903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c29</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>7.867643</td>\n",
       "      <td>6.002377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c35</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>9.238033</td>\n",
       "      <td>6.190196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c36</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>10.347633</td>\n",
       "      <td>5.920125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c37</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>9.200105</td>\n",
       "      <td>5.879345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c38</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>2.912634</td>\n",
       "      <td>3.486267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c39</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c40</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>0.027465</td>\n",
       "      <td>0.316193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c41</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>1.405702</td>\n",
       "      <td>2.934206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c42</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>6.456709</td>\n",
       "      <td>6.505373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c43</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>7.187287</td>\n",
       "      <td>6.469061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c44</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>7.921528</td>\n",
       "      <td>6.316368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c45</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>8.674863</td>\n",
       "      <td>5.805924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c46</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>3.510332</td>\n",
       "      <td>4.369131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c47</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>0.019880</td>\n",
       "      <td>0.213668</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c48</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>0.017787</td>\n",
       "      <td>0.269110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c49</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>0.820037</td>\n",
       "      <td>2.009018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c50</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>7.868951</td>\n",
       "      <td>5.666636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c51</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>9.885692</td>\n",
       "      <td>5.141561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c52</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>9.764844</td>\n",
       "      <td>5.314977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c53</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>9.283285</td>\n",
       "      <td>5.940887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c54</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>3.743918</td>\n",
       "      <td>4.901657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c55</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>0.148313</td>\n",
       "      <td>0.767761</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c56</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.016173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c57</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>0.283024</td>\n",
       "      <td>0.928046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c58</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>5.855872</td>\n",
       "      <td>4.980012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c59</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>11.942977</td>\n",
       "      <td>4.334508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c60</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>11.461156</td>\n",
       "      <td>4.991934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c61</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>6.700497</td>\n",
       "      <td>5.775815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c62</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>2.105676</td>\n",
       "      <td>4.028266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c63</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>0.202197</td>\n",
       "      <td>1.150694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c64</th>\n",
       "      <td>3823.0</td>\n",
       "      <td>4.497253</td>\n",
       "      <td>2.869831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      count       mean       std  min   25%   50%   75%   max\n",
       "c00  3823.0   0.000000  0.000000  0.0   0.0   0.0   0.0   0.0\n",
       "c01  3823.0   0.301334  0.866986  0.0   0.0   0.0   0.0   8.0\n",
       "c02  3823.0   5.481821  4.631601  0.0   1.0   5.0   9.0  16.0\n",
       "c03  3823.0  11.805912  4.259811  0.0  10.0  13.0  15.0  16.0\n",
       "c04  3823.0  11.451478  4.537556  0.0   9.0  13.0  15.0  16.0\n",
       "c05  3823.0   5.505362  5.613060  0.0   0.0   4.0  10.0  16.0\n",
       "c06  3823.0   1.387392  3.371444  0.0   0.0   0.0   0.0  16.0\n",
       "c07  3823.0   0.142297  1.051598  0.0   0.0   0.0   0.0  16.0\n",
       "c08  3823.0   0.002093  0.088572  0.0   0.0   0.0   0.0   5.0\n",
       "c09  3823.0   1.960502  3.052353  0.0   0.0   0.0   3.0  15.0\n",
       "c10  3823.0  10.577295  5.435481  0.0   7.0  13.0  15.0  16.0\n",
       "c11  3823.0  11.715407  4.012160  0.0   9.0  13.0  15.0  16.0\n",
       "c12  3823.0  10.624902  4.788136  0.0   8.0  12.0  15.0  16.0\n",
       "c13  3823.0   8.295579  5.935551  0.0   2.0   9.0  14.0  16.0\n",
       "c14  3823.0   2.200105  4.062178  0.0   0.0   0.0   3.0  16.0\n",
       "c15  3823.0   0.151975  0.988778  0.0   0.0   0.0   0.0  15.0\n",
       "c16  3823.0   0.004970  0.119857  0.0   0.0   0.0   0.0   5.0\n",
       "c17  3823.0   2.595867  3.454065  0.0   0.0   1.0   4.0  16.0\n",
       "c18  3823.0   9.580696  5.886126  0.0   4.0  11.0  15.0  16.0\n",
       "c19  3823.0   6.735025  5.918303  0.0   1.0   5.0  12.0  16.0\n",
       "c20  3823.0   7.186503  6.142687  0.0   1.0   6.0  13.0  16.0\n",
       "c21  3823.0   8.048391  6.291498  0.0   0.0   9.0  14.0  16.0\n",
       "c22  3823.0   2.046037  3.581740  0.0   0.0   0.0   3.0  16.0\n",
       "c23  3823.0   0.049176  0.435462  0.0   0.0   0.0   0.0   8.0\n",
       "c24  3823.0   0.001046  0.032334  0.0   0.0   0.0   0.0   1.0\n",
       "c25  3823.0   2.335600  3.085915  0.0   0.0   0.0   4.0  16.0\n",
       "c26  3823.0   9.239079  6.128091  0.0   3.0  11.0  15.0  16.0\n",
       "c27  3823.0   9.133665  5.902591  0.0   4.0  10.0  15.0  16.0\n",
       "c28  3823.0   9.673293  6.282903  0.0   3.0  12.0  16.0  16.0\n",
       "c29  3823.0   7.867643  6.002377  0.0   1.0   8.0  14.0  16.0\n",
       "..      ...        ...       ...  ...   ...   ...   ...   ...\n",
       "c35  3823.0   9.238033  6.190196  0.0   3.0  11.0  16.0  16.0\n",
       "c36  3823.0  10.347633  5.920125  0.0   5.0  12.0  16.0  16.0\n",
       "c37  3823.0   9.200105  5.879345  0.0   4.0  11.0  15.0  16.0\n",
       "c38  3823.0   2.912634  3.486267  0.0   0.0   1.0   6.0  14.0\n",
       "c39  3823.0   0.000000  0.000000  0.0   0.0   0.0   0.0   0.0\n",
       "c40  3823.0   0.027465  0.316193  0.0   0.0   0.0   0.0   7.0\n",
       "c41  3823.0   1.405702  2.934206  0.0   0.0   0.0   1.0  16.0\n",
       "c42  3823.0   6.456709  6.505373  0.0   0.0   4.0  13.0  16.0\n",
       "c43  3823.0   7.187287  6.469061  0.0   0.0   7.0  14.0  16.0\n",
       "c44  3823.0   7.921528  6.316368  0.0   1.0   8.0  15.0  16.0\n",
       "c45  3823.0   8.674863  5.805924  0.0   3.0  10.0  14.0  16.0\n",
       "c46  3823.0   3.510332  4.369131  0.0   0.0   1.0   7.0  16.0\n",
       "c47  3823.0   0.019880  0.213668  0.0   0.0   0.0   0.0   6.0\n",
       "c48  3823.0   0.017787  0.269110  0.0   0.0   0.0   0.0  10.0\n",
       "c49  3823.0   0.820037  2.009018  0.0   0.0   0.0   0.0  16.0\n",
       "c50  3823.0   7.868951  5.666636  0.0   2.0   8.0  13.0  16.0\n",
       "c51  3823.0   9.885692  5.141561  0.0   6.0  10.0  15.0  16.0\n",
       "c52  3823.0   9.764844  5.314977  0.0   5.0  10.0  15.0  16.0\n",
       "c53  3823.0   9.283285  5.940887  0.0   4.0  11.0  15.0  16.0\n",
       "c54  3823.0   3.743918  4.901657  0.0   0.0   1.0   7.0  16.0\n",
       "c55  3823.0   0.148313  0.767761  0.0   0.0   0.0   0.0  12.0\n",
       "c56  3823.0   0.000262  0.016173  0.0   0.0   0.0   0.0   1.0\n",
       "c57  3823.0   0.283024  0.928046  0.0   0.0   0.0   0.0  10.0\n",
       "c58  3823.0   5.855872  4.980012  0.0   1.0   5.0  10.0  16.0\n",
       "c59  3823.0  11.942977  4.334508  0.0  10.0  13.0  15.0  16.0\n",
       "c60  3823.0  11.461156  4.991934  0.0   9.0  13.0  16.0  16.0\n",
       "c61  3823.0   6.700497  5.775815  0.0   0.0   6.0  12.0  16.0\n",
       "c62  3823.0   2.105676  4.028266  0.0   0.0   0.0   2.0  16.0\n",
       "c63  3823.0   0.202197  1.150694  0.0   0.0   0.0   0.0  16.0\n",
       "c64  3823.0   4.497253  2.869831  0.0   2.0   4.0   7.0   9.0\n",
       "\n",
       "[65 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_XY_tv.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Visualizando los datos\n",
    "\n",
    "Para visualizar los datos utilizaremos el método `imshow` de `matplotlib`. Resulta necesario convertir el arreglo desde las dimensiones (1,64)  a (8,8) para que la imagen sea cuadrada y pueda distinguirse el dígito. Superpondremos además el label correspondiente al dígito, mediante el método `text`. Esto nos permitirá comparar la imagen generada con la etiqueta asociada a los valores. Realizaremos lo anterior para los primeros 25 datos del archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Well plot the first nx*ny examples\n",
    "nx, ny = 5, 5\n",
    "fig, ax = plt.subplots(nx, ny, figsize=(12,12))\n",
    "for i in range(nx):\n",
    "    for j in range(ny):\n",
    "        index = j+ny*i\n",
    "        data  = X_tv[index,:].reshape(8,8)\n",
    "        label = Y_tv[index]\n",
    "        ax[i][j].imshow(data, interpolation='nearest', cmap=plt.get_cmap('gray_r'))\n",
    "        ax[i][j].text(7, 0, str(int(label)), horizontalalignment='center',\n",
    "                verticalalignment='center', fontsize=10, color='blue')\n",
    "        ax[i][j].get_xaxis().set_visible(False)\n",
    "        ax[i][j].get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entrenando el modelo\n",
    "\n",
    "#### 2.1 Entrenamiento trivial\n",
    "\n",
    "Entrenaremos el modelo con 1 vecino y verificaremos el error de predicción en el set de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 0 errores de un total de 3823 ejemplos de entrenamiento\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k = 1\n",
    "kNN = KNeighborsClassifier(n_neighbors=k)\n",
    "kNN.fit(X_tv, Y_tv)\n",
    "Y_pred = kNN.predict(X_tv)\n",
    "n_errors = sum(Y_pred!=Y_tv)\n",
    "print(\"Hay %d errores de un total de %d ejemplos de entrenamiento\" %(n_errors, len(Y_tv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desafío 1\n",
    "¿Porqué el error de entrenamiento es 0 en el modelo?\n",
    "\n",
    "***RESPONDA AQUI***\n",
    "\n",
    "Porque en kNN el elemento más cercano es el mismo punto que buscamos predecir, y conocemos la etiqueta exacta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Buscando el valor de k más apropiado\n",
    "\n",
    "A partir del análisis del punto anterior, nos damos cuenta de la necesidad de:\n",
    "1. Calcular el error en un set distinto al utilizado para entrenar.\n",
    "2. Calcular el mejor valor de vecinos para el algoritmo.\n",
    "\n",
    "## Desafío 2\n",
    "\n",
    "Complete el código entregado a continuación, de modo que se calcule el error de predicción como el porcentaje de aciertos de kNN, para k entre 1 y 10 (ambos incluidos). \n",
    "\n",
    "Realice una división en set de entrenamiento (75%) y de validación (25%), y calcule el valor promedio y desviación estándar del error de predicción (en porcentaje), tomando al menos 20 repeticiones para cada valor de k.\n",
    "\n",
    "OBS: Ejecución de la celda debería tomar alrededor de 5 minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paciencia. Debería tomar algunos minutos.\n",
      "k=1: 1.62 +- 0.34 errores de clasificación de un total de 956 puntos\n",
      "k=2: 2.12 +- 0.38 errores de clasificación de un total de 956 puntos\n",
      "k=3: 1.48 +- 0.33 errores de clasificación de un total de 956 puntos\n",
      "k=4: 1.75 +- 0.38 errores de clasificación de un total de 956 puntos\n",
      "k=5: 1.59 +- 0.33 errores de clasificación de un total de 956 puntos\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "template = \"k={0:,d}: {1:.1f} +- {2:.1f} errores de clasificación de un total de {3:,d} puntos\"\n",
    "# Splitting the data\n",
    "X_train, X_valid, Y_train, Y_valid = ##FIX ME##\n",
    "# Fitting the model\n",
    "mean_error_for_k = []\n",
    "std_error_for_k = []\n",
    "k_range = ##FIX ME##\n",
    "for k in k_range:\n",
    "    errors_k = []\n",
    "    for i in ##FIX ME##:\n",
    "        kNN = ##FIX ME##\n",
    "        kNN.fit(X_train, Y_train)\n",
    "        # Predicting values\n",
    "        Y_valid_pred = kNN.predict(X_valid)\n",
    "        # Count the errors\n",
    "        n_errors = ##FIX ME## \n",
    "        # Add them to vector\n",
    "        errors_k.append(100.*n_errors/len(Y_valid))\n",
    "\n",
    "    errors = np.array(errors_k)\n",
    "    print template.format(k, errors.mean(), errors.std(), len(Y_valid))\n",
    "    mean_error_for_k.append(errors.mean())\n",
    "    std_error_for_k.append(errors.std())\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split # Spoiler alert #\n",
    "\n",
    "print(\"Paciencia. Debería tomar algunos minutos.\")\n",
    "template = \"k={0:,d}: {1:.2f} +- {2:.2f} errores de clasificación de un total de {3:,d} puntos\"\n",
    "# Fitting the model\n",
    "mean_error_for_k = []\n",
    "std_error_for_k = []\n",
    "k_range = range(1,11)\n",
    "for k in k_range:\n",
    "    errors_k = []\n",
    "    for i in range(21):\n",
    "        # Splitting the data\n",
    "        X_train, X_valid, Y_train, Y_valid = train_test_split(X_tv, Y_tv, test_size=0.25, random_state=i)\n",
    "        # Training the model\n",
    "        kNN = KNeighborsClassifier(n_neighbors=k)\n",
    "        kNN.fit(X_train, Y_train)\n",
    "        # Predicting values\n",
    "        Y_valid_pred = kNN.predict(X_valid)\n",
    "        # Count the errors\n",
    "        n_errors = sum(Y_valid_pred!=Y_valid)\n",
    "        # Add them to vector\n",
    "        errors_k.append(100.*n_errors/len(Y_valid))\n",
    "\n",
    "    errors = np.array(errors_k)\n",
    "    print(template.format(k, errors.mean(), errors.std(), len(Y_valid)))\n",
    "    mean_error_for_k.append(errors.mean())\n",
    "    std_error_for_k.append(errors.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observación: El código anterior debería dar un resultado similar al siguiente:\n",
    "```\n",
    "k=1: 1.62 +- 0.34 errores de clasificación de un total de 956 puntos\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Visualizado el error de predicción\n",
    "Podemos visualizar los datos anteriores utilizando el siguiente código, que requiere que `sd_error_for k` y `mean_error_for_k` hayan sido apropiadamente definidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = np.array(mean_error_for_k)\n",
    "std = np.array(std_error_for_k)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(k_range, mean - std, \"k:\")\n",
    "plt.plot(k_range, mean , \"r.-\")\n",
    "plt.plot(k_range, mean + std, \"k:\")\n",
    "plt.xlabel(\"Numero de vecinos k\")\n",
    "plt.ylabel(\"Error de clasificacion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desafío 3\n",
    "¿Qué patrón se observa en los datos, en relación a los números pares e impares? ¿Porqué sucede esto? ¿Qué valor de $k$ elegirá para el algoritmo?\n",
    "\n",
    "***RESPONDA AQUI***\n",
    "\n",
    "Vemos que los puntos impares tienen valores de error más bajos que sus vecinos pares. Esto se produce puesto que es más facil romper los empates que se pueden formar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Entrenando con todos los datos\n",
    "\n",
    "A partir de lo anterior, se fija el número de vecinos $k$ y se procede a entrenar el modelo con todos los datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "k = 3 # Fix here, maybe\n",
    "kNN = KNeighborsClassifier(n_neighbors=k)\n",
    "kNN.fit(X_tv, Y_tv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Predicción en testing dataset\n",
    "\n",
    "Ahora que el modelo kNN ha sido completamente entrenado, calcularemos el error de predicción en un set de datos completamente nuevo: el set de testing. \n",
    "\n",
    "## Desafío 4\n",
    "Complete el código a continuación, para cargar los datos del set de entrenamiento y realizar una predicción de los dígitos de cada imagen. ***No cambie los nombres de las variables***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Cargando el archivo data/optdigits.tes\n",
    "XY_test = ##FIX ME##\n",
    "X_test = ##FIX ME##\n",
    "Y_test = ##FIX ME##\n",
    "# Predicción de etiquetas\n",
    "Y_pred = ##FIX ME##\n",
    "\"\"\"\n",
    "# Cargando el archivo data/optdigits.tes\n",
    "filepath = os.path.join(\"data\", \"optdigits.test\")\n",
    "XY_test = np.loadtxt(filepath, delimiter=\",\", dtype=np.int8)\n",
    "X_test = XY_tv[:,:64]\n",
    "Y_test = XY_tv[:, 64]\n",
    "# Predicción de etiquetas\n",
    "Y_pred = kNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Visualización de etiquetas correctas\n",
    "Puesto que tenemos las etiquetas verdaderas en el set de test, podemos visualizar únicamente los números que han sido correctamente etiquetados. Ejecute el código a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Mostrar los datos correctos\n",
    "mask = (Y_pred==Y_test)\n",
    "X_aux = X_test[mask]\n",
    "Y_aux_true = Y_test[mask]\n",
    "Y_aux_pred = Y_pred[mask]\n",
    "\n",
    "# We'll plot the first 100 examples, randomly choosen\n",
    "nx, ny = 5, 5\n",
    "fig, ax = plt.subplots(nx, ny, figsize=(12,12))\n",
    "for i in range(nx):\n",
    "    for j in range(ny):\n",
    "        index = j+ny*i\n",
    "        data  = X_aux[index,:].reshape(8,8)\n",
    "        label_pred = str(int(Y_aux_pred[index]))\n",
    "        label_true = str(int(Y_aux_true[index]))\n",
    "        ax[i][j].imshow(data, interpolation='nearest', cmap=plt.get_cmap('gray_r'))\n",
    "        ax[i][j].text(0, 0, label_pred, horizontalalignment='center',\n",
    "                verticalalignment='center', fontsize=10, color='green')\n",
    "        ax[i][j].text(7, 0, label_true, horizontalalignment='center',\n",
    "                verticalalignment='center', fontsize=10, color='blue')\n",
    "        ax[i][j].get_xaxis().set_visible(False)\n",
    "        ax[i][j].get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Visualización de etiquetas incorrectas\n",
    "Más interesante que el gráfico anterior, resulta considerar los casos donde los dígitos han sido incorrectamente etiquetados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desafio 5\n",
    "Modifique el código anteriormente provisto para que muestre los dígitos incorrectamente etiquetados, cambiando apropiadamente la máscara. Cambie también el color de la etiqueta desde verde a rojo, para indicar una mala etiquetación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "##FIX ME##\n",
    "\"\"\"\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Mostrar los datos correctos\n",
    "mask = (Y_pred!=Y_test)\n",
    "X_aux = X_test[mask]\n",
    "Y_aux_true = Y_test[mask]\n",
    "Y_aux_pred = Y_pred[mask]\n",
    "\n",
    "# We'll plot the first 100 examples, randomly choosen\n",
    "nx, ny = 5, 5\n",
    "fig, ax = plt.subplots(nx, ny, figsize=(12,12))\n",
    "for i in range(nx):\n",
    "    for j in range(ny):\n",
    "        index = j+ny*i\n",
    "        data  = X_aux[index,:].reshape(8,8)\n",
    "        label_pred = str(int(Y_aux_pred[index]))\n",
    "        label_true = str(int(Y_aux_true[index]))\n",
    "        ax[i][j].imshow(data, interpolation='nearest', cmap=plt.get_cmap('gray_r'))\n",
    "        ax[i][j].text(0, 0, label_pred, horizontalalignment='center',\n",
    "                verticalalignment='center', fontsize=10, color='red')\n",
    "        ax[i][j].text(7, 0, label_true, horizontalalignment='center',\n",
    "                verticalalignment='center', fontsize=10, color='blue')\n",
    "        ax[i][j].get_xaxis().set_visible(False)\n",
    "        ax[i][j].get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8 Análisis del error\n",
    "\n",
    "Después de la exploración visual de los resultados, estamos ansiosos de obtener el error de predicción real del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desafío 6\n",
    "Complete el código, obteniendo el error de clasificación para cada dígito. Indique el error obtenido en su respuesta.\n",
    "¿Existen dígitos más fáciles o difíciles de clasificar?\n",
    "\n",
    "***RESPONDER AQUI***\n",
    "\n",
    "Si, y depende del valor elegido para k y de la métrica utilizada. Pero en general, vemos que los números que más se confunden son el 9 y el 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Error global\n",
    "mask = (Y_pred!=Y_test)\n",
    "error_prediccion = ##FIX ME##\n",
    "print \"Error de predicción total de %.1f \" %error_prediccion\n",
    "\n",
    "for digito in range(0,10):\n",
    "    mask_digito = ##FIX ME##\n",
    "    Y_test_digito = Y_test[mask_digito] \n",
    "    Y_pred_digito = Y_pred[mask_digito]\n",
    "    error_prediccion = 100.*sum((Y_pred_digito!=Y_test_digito)) / len(Y_pred_digito)\n",
    "    print \"Error de predicción para digito %d de %.1f \" %(digito, error_prediccion)\n",
    "\"\"\"\n",
    "# Error global\n",
    "mask = (Y_pred!=Y_test)\n",
    "error_prediccion = 100*sum(mask) / len(Y_pred)\n",
    "print(\"Error de predicción total de {:.1f}%\".format(error_prediccion))\n",
    "\n",
    "for digito in range(0,10):\n",
    "    mask_digito = Y_test==digito\n",
    "    Y_test_digito = Y_test[mask_digito] \n",
    "    Y_pred_digito = Y_pred[mask_digito]\n",
    "    error_prediccion = 100.*sum((Y_pred_digito!=Y_test_digito)) / len(Y_pred_digito)\n",
    "    print(\"Error de predicción para digito {} de {:.1f}%\".format(digito, error_prediccion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9 Análisis del error (cont. de)\n",
    "\n",
    "La matriz de confusión (confusion matrix) indica la relación entre las etiquetas clasificadas correcta e incorrectamente:\n",
    "\n",
    "*Compute confusion matrix to evaluate the accuracy of a classification*\n",
    "*By definition a confusion matrix $C$ is such that $C_{i,j}$ is equal to the number of observations known to be in group $i$ but predicted to be in group $j$.* \n",
    "\n",
    "Es decir, el elemento $C_{3,3}$ cuenta la cantidad de veces que el dígito 3 fue clasificado correctamente, mientras que $C_{9,7}$ indica la cantidad de veces que el dígito 9 fue incorrectamente clasificado como el digito 7.\n",
    "\n",
    "Observación: Lo anterior corresponde a la convención utilizada por sklearn. La convención puede variar según la referencia.\n",
    "\n",
    "El siguiente código muestra cómo clasificar el error de clasificación con la matriz de confusión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As in http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix (withou)', cmap=plt.cm.gray_r):\n",
    "    cm_aux = cm - np.diag(np.diag(cm))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(cm_aux, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(10)\n",
    "    plt.xticks(tick_marks, tick_marks)\n",
    "    plt.yticks(tick_marks, tick_marks)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "# Compute confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desafío 7\n",
    "¿Qué puede observarse a partir de la matriz de confusión anterior?\n",
    "\n",
    "\n",
    "***RESPONDER AQUI***\n",
    "\n",
    "Se observa claramente que la etiqueta más incorrectamente clasificada es para el dígito 8, que se clasifica incorrectamente como 1."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
